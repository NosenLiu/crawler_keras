<!DOCTYPE html><!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]--><!--[if gt IE 8]><!--><html class=" js flexbox flexboxlegacy canvas canvastext no-webgl touch no-geolocation postmessage websqldatabase indexeddb hashchange history draganddrop websockets rgba hsla multiplebgs backgroundsize borderimage borderradius boxshadow textshadow opacity cssanimations csscolumns cssgradients cssreflections csstransforms csstransforms3d csstransitions fontface generatedcontent no-video no-audio localstorage sessionstorage webworkers applicationcache svg inlinesvg smil svgclippaths" lang="en" style=""><!--<![endif]--><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <meta name="author" content="BigMoyan">
  
  <title>函数式模型 - Keras中文文档</title>
  

  <link rel="shortcut icon" href="../../img/favicon.ico">

  
  <link href="https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700" rel="stylesheet" type="text/css">

  <link rel="stylesheet" href="../../css/theme.css" type="text/css">
  <link rel="stylesheet" href="../../css/theme_extra.css" type="text/css">
  <link rel="stylesheet" href="../../css/highlight.css">
  <link href="https://media.readthedocs.org/css/badge_only.css" rel="stylesheet">
  <link href="https://media.readthedocs.org/css/readthedocs-doc-embed.css" rel="stylesheet">

  
  <script async="" src="https://www.google-analytics.com/analytics.js"></script><script>
    // Current page data
    var mkdocs_page_name = "函数式模型";
    var mkdocs_page_input_path = "getting_started/functional_API.md";
    var mkdocs_page_url = "/getting_started/functional_API/";
  </script>
  
  <script src="../../js/jquery-2.1.1.min.js"></script>
  <script src="../../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../../js/highlight.pack.js"></script>
  <script src="../../js/theme.js"></script> 
  <script src="../../readthedocs-data.js"></script>
  <script src="https://media.readthedocs.org/static/core/js/readthedocs-doc-embed.js"></script>
  <script src="https://media.readthedocs.org/javascript/readthedocs-analytics.js"></script>

  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../.." class="icon icon-home"> Keras中文文档</a>
        <div role="search">
  <form id="mkdocs-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs">
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        <ul class="current">
          
            <li>
    </li><li class="toctree-l1 ">
        <a class="" href="./../..info.html">主页</a>
        
    </li>
<li>
          
            </li><li>
    <ul class="subnav">
    <li><span>keras新手指南</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../for_beginners/concepts/info.html">一些基本概念</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../for_beginners/FAQ/info.html">常见问题与解答</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../for_beginners/keras_linux/info.html">Keras安装和配置指南(Linux)</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../for_beginners/keras_windows/info.html">Keras安装和配置指南(Windows)</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../for_beginners/trap/info.html">Keras使用陷阱</a>
        
    </li>

        
    </ul>
</li><li>
          
            </li><li>
    <ul class="subnav">
    <li><span>快速开始</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../sequential_model/info.html">序贯模型</a>
        
    </li>

        
            
    <li class="toctree-l1 current">
        <a class="current" href="././info.html">函数式模型</a>
        
            <ul>
            
                <li class="toctree-l3"><a href="#functional">快速开始函数式（Functional）模型</a></li>
                
                    <li><a class="toctree-l4" href="#_1">第一个模型：全连接网络</a></li>
                
                    <li><a class="toctree-l4" href="#_2">所有的模型都是可调用的，就像层一样</a></li>
                
                    <li><a class="toctree-l4" href="#_3">多输入和多输出模型</a></li>
                
                    <li><a class="toctree-l4" href="#_4">共享层</a></li>
                
                    <li><a class="toctree-l4" href="#_5">层“节点”的概念</a></li>
                
                    <li><a class="toctree-l4" href="#_6">更多的例子</a></li>
                
            
            </ul>
        
    </li>

        
    </ul>
</li><li>
          
            </li><li>
    <ul class="subnav">
    <li><span>模型</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../models/about_model/info.html">关于Keras模型</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../models/sequential/info.html">序贯模型API</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../models/model/info.html">函数式模型API</a>
        
    </li>

        
    </ul>
</li><li>
          
            </li><li>
    <ul class="subnav">
    <li><span>网络层</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../layers/about_layer/info.html">关于Keras层</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../layers/core_layer/info.html">常用层Core</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../layers/convolutional_layer/info.html">卷积层Convolutional</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../layers/pooling_layer/info.html">池化层Pooling</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../layers/locally_connected_layer/info.html">局部连接层Locally-connented</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../layers/recurrent_layer/info.html">循环层Recurrent</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../layers/embedding_layer/info.html">嵌入层Embedding</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../layers/merge/info.html">融合层Merge</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../layers/advanced_activation_layer/info.html">高级激活层Advanced Activation</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../layers/normalization_layer/info.html">规范层BatchNormalization</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../layers/noise_layer/info.html">噪声层Noise</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../layers/wrapper/info.html">包装器Wrapper</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../layers/writting_layer/info.html">编写自己的层</a>
        
    </li>

        
    </ul>
</li><li>
          
            </li><li>
    <ul class="subnav">
    <li><span>数据预处理</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../preprocessing/sequence/info.html">序列预处理</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../preprocessing/text/info.html">文本预处理</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../preprocessing/image/info.html">图片预处理</a>
        
    </li>

        
    </ul>
</li><li>
          
            </li><li>
    <ul class="subnav">
    <li><span>网络配置</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../other/objectives/info.html">损失函数loss</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../other/optimizers/info.html">优化器Optimizer</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../other/activations/info.html">激活函数Activation</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../other/metrics/info.html">性能评估Metrices</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../other/initializations/info.html">初始化方法Initializers</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../other/regularizers/info.html">正则项Regularizer</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../other/constraints/info.html">约束项Constraint</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../other/callbacks/info.html">回调函数Callback</a>
        
    </li>

        
    </ul>
</li><li>
          
            </li><li>
    <ul class="subnav">
    <li><span>协助使用Keras</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../other/application/info.html">预训练模型Application</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../other/datasets/info.html">常用数据库Dataset</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../other/visualization/info.html">可视化visualization</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../utils/info.html">工具</a>
        
    </li>

        
    </ul>
</li><li>
          
            </li><li>
    </li><li class="toctree-l1 ">
        <a class="" href="./../../backend/info.html">keras后端Backend</a>
        
    </li>
<li>
          
            </li><li>
    </li><li class="toctree-l1 ">
        <a class="" href="./../../scikit-learn_API/info.html">scikit-learn接口</a>
        
    </li>
<li>
          
            </li><li>
    </li><li class="toctree-l1 ">
        <a class="" href="./../../acknowledgement/info.html">致谢</a>
        
    </li>
<li>
          
        </li></ul>
      </div>
      &nbsp;
    <div id="rtd-hivxilkx" class="ethical-rtd"></div></nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../..">Keras中文文档</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../..">Docs</a> »</li>
    
      
        
          <li>快速开始 »</li>
        
      
    
    <li>函数式模型</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="functional">快速开始函数式（Functional）模型</h1>
<p>我们起初将Functional一词译作泛型，想要表达该类模型能够表达任意张量映射的含义，但表达的不是很精确，在Keras 2里我们将这个词改译为“函数式”，对函数式编程有所了解的同学应能够快速get到该类模型想要表达的含义。函数式模型称作Functional，但它的类名是Model，因此我们有时候也用Model来代表函数式模型。</p>
<p>Keras函数式模型接口是用户定义多输出模型、非循环有向模型或具有共享层的模型等复杂模型的途径。一句话，只要你的模型不是类似VGG一样一条路走到黑的模型，或者你的模型需要多于一个的输出，那么你总应该选择函数式模型。函数式模型是最广泛的一类模型，序贯模型（Sequential）只是它的一种特殊情况。</p>
<p>这部分的文档假设你已经对Sequential模型已经比较熟悉</p>
<p>让我们从简单一点的模型开始</p>
<h2 id="_1">第一个模型：全连接网络</h2>
<p><code>Sequential</code>当然是实现全连接网络的最好方式，但我们从简单的全连接网络开始，有助于我们学习这部分的内容。在开始前，有几个概念需要澄清：</p>
<ul>
<li>
<p>层对象接受张量为参数，返回一个张量。</p>
</li>
<li>
<p>输入是张量，输出也是张量的一个框架就是一个模型，通过<code>Model</code>定义。</p>
</li>
<li>
<p>这样的模型可以被像Keras的<code>Sequential</code>一样被训练</p>
</li>
</ul>
<pre><code class="python hljs"><span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> Input, Dense
<span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> Model

<span class="hljs-comment"># This returns a tensor</span>
inputs = Input(shape=(<span class="hljs-number">784</span>,))

<span class="hljs-comment"># a layer instance is callable on a tensor, and returns a tensor</span>
x = Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">'relu'</span>)(inputs)
x = Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">'relu'</span>)(x)
predictions = Dense(<span class="hljs-number">10</span>, activation=<span class="hljs-string">'softmax'</span>)(x)

<span class="hljs-comment"># This creates a model that includes</span>
<span class="hljs-comment"># the Input layer and three Dense layers</span>
model = Model(inputs=inputs, outputs=predictions)
model.compile(optimizer=<span class="hljs-string">'rmsprop'</span>,
              loss=<span class="hljs-string">'categorical_crossentropy'</span>,
              metrics=[<span class="hljs-string">'accuracy'</span>])
model.fit(data, labels)  <span class="hljs-comment"># starts training</span>
</code></pre>

<hr>
<h2 id="_2">所有的模型都是可调用的，就像层一样</h2>
<p>利用函数式模型的接口，我们可以很容易的重用已经训练好的模型：你可以把模型当作一个层一样，通过提供一个tensor来调用它。注意当你调用一个模型时，你不仅仅重用了它的结构，也重用了它的权重。</p>
<pre><code class="python hljs">x = Input(shape=(<span class="hljs-number">784</span>,))
<span class="hljs-comment"># This works, and returns the 10-way softmax we defined above.</span>
y = model(x)
</code></pre>

<p>这种方式可以允许你快速的创建能处理序列信号的模型，你可以很快将一个图像分类的模型变为一个对视频分类的模型，只需要一行代码：</p>
<pre><code class="python hljs"><span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> TimeDistributed

<span class="hljs-comment"># Input tensor for sequences of 20 timesteps,</span>
<span class="hljs-comment"># each containing a 784-dimensional vector</span>
input_sequences = Input(shape=(<span class="hljs-number">20</span>, <span class="hljs-number">784</span>))

<span class="hljs-comment"># This applies our previous model to every timestep in the input sequences.</span>
<span class="hljs-comment"># the output of the previous model was a 10-way softmax,</span>
<span class="hljs-comment"># so the output of the layer below will be a sequence of 20 vectors of size 10.</span>
processed_sequences = TimeDistributed(model)(input_sequences)
</code></pre>

<hr>
<h2 id="_3">多输入和多输出模型</h2>
<p>使用函数式模型的一个典型场景是搭建多输入、多输出的模型。</p>
<p>考虑这样一个模型。我们希望预测Twitter上一条新闻会被转发和点赞多少次。模型的主要输入是新闻本身，也就是一个词语的序列。但我们还可以拥有额外的输入，如新闻发布的日期等。这个模型的损失函数将由两部分组成，辅助的损失函数评估仅仅基于新闻本身做出预测的情况，主损失函数评估基于新闻和额外信息的预测的情况，即使来自主损失函数的梯度发生弥散，来自辅助损失函数的信息也能够训练Embeddding和LSTM层。在模型中早点使用主要的损失函数是对于深度网络的一个良好的正则方法。总而言之，该模型框图如下：</p>
<p><img alt="multi-input-multi-output-graph" src="../../images/multi-input-multi-output-graph.png"></p>
<p>让我们用函数式模型来实现这个框图</p>
<p>主要的输入接收新闻本身，即一个整数的序列（每个整数编码了一个词）。这些整数位于1到10，000之间（即我们的字典有10，000个词）。这个序列有100个单词。</p>
<pre><code class="python hljs"><span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> Input, Embedding, LSTM, Dense
<span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> Model

<span class="hljs-comment"># Headline input: meant to receive sequences of 100 integers, between 1 and 10000.</span>
<span class="hljs-comment"># Note that we can name any layer by passing it a "name" argument.</span>
main_input = Input(shape=(<span class="hljs-number">100</span>,), dtype=<span class="hljs-string">'int32'</span>, name=<span class="hljs-string">'main_input'</span>)

<span class="hljs-comment"># This embedding layer will encode the input sequence</span>
<span class="hljs-comment"># into a sequence of dense 512-dimensional vectors.</span>
x = Embedding(output_dim=<span class="hljs-number">512</span>, input_dim=<span class="hljs-number">10000</span>, input_length=<span class="hljs-number">100</span>)(main_input)

<span class="hljs-comment"># A LSTM will transform the vector sequence into a single vector,</span>
<span class="hljs-comment"># containing information about the entire sequence</span>
lstm_out = LSTM(<span class="hljs-number">32</span>)(x)
</code></pre>

<p>然后，我们插入一个额外的损失，使得即使在主损失很高的情况下，LSTM和Embedding层也可以平滑的训练。</p>
<pre><code class="python hljs">auxiliary_output = Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">'sigmoid'</span>, name=<span class="hljs-string">'aux_output'</span>)(lstm_out)
</code></pre>

<p>再然后，我们将LSTM与额外的输入数据串联起来组成输入，送入模型中：</p>
<pre><code class="python hljs">auxiliary_input = Input(shape=(<span class="hljs-number">5</span>,), name=<span class="hljs-string">'aux_input'</span>)
x = keras.layers.concatenate([lstm_out, auxiliary_input])

<span class="hljs-comment"># We stack a deep densely-connected network on top</span>
x = Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">'relu'</span>)(x)
x = Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">'relu'</span>)(x)
x = Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">'relu'</span>)(x)

<span class="hljs-comment"># And finally we add the main logistic regression layer</span>
main_output = Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">'sigmoid'</span>, name=<span class="hljs-string">'main_output'</span>)(x)
</code></pre>

<p>最后，我们定义整个2输入，2输出的模型：</p>
<pre><code class="python hljs">model = Model(inputs=[main_input, auxiliary_input], outputs=[main_output, auxiliary_output])
</code></pre>

<p>模型定义完毕，下一步编译模型。我们给额外的损失赋0.2的权重。我们可以通过关键字参数<code>loss_weights</code>或<code>loss</code>来为不同的输出设置不同的损失函数或权值。这两个参数均可为Python的列表或字典。这里我们给<code>loss</code>传递单个损失函数，这个损失函数会被应用于所有输出上。</p>
<pre><code class="python hljs">model.compile(optimizer=<span class="hljs-string">'rmsprop'</span>, loss=<span class="hljs-string">'binary_crossentropy'</span>,
              loss_weights=[<span class="hljs-number">1.</span>, <span class="hljs-number">0.2</span>])
</code></pre>

<p>编译完成后，我们通过传递训练数据和目标值训练该模型：</p>
<pre><code class="python hljs">model.fit([headline_data, additional_data], [labels, labels],
          epochs=<span class="hljs-number">50</span>, batch_size=<span class="hljs-number">32</span>)

</code></pre>

<p>因为我们输入和输出是被命名过的（在定义时传递了“name”参数），我们也可以用下面的方式编译和训练模型：</p>
<pre><code class="python hljs">model.compile(optimizer=<span class="hljs-string">'rmsprop'</span>,
              loss={<span class="hljs-string">'main_output'</span>: <span class="hljs-string">'binary_crossentropy'</span>, <span class="hljs-string">'aux_output'</span>: <span class="hljs-string">'binary_crossentropy'</span>},
              loss_weights={<span class="hljs-string">'main_output'</span>: <span class="hljs-number">1.</span>, <span class="hljs-string">'aux_output'</span>: <span class="hljs-number">0.2</span>})

<span class="hljs-comment"># And trained it via:</span>
model.fit({<span class="hljs-string">'main_input'</span>: headline_data, <span class="hljs-string">'aux_input'</span>: additional_data},
          {<span class="hljs-string">'main_output'</span>: labels, <span class="hljs-string">'aux_output'</span>: labels},
          epochs=<span class="hljs-number">50</span>, batch_size=<span class="hljs-number">32</span>)
</code></pre>

<hr>
<p><a name="node">
<font color="#404040">        </font></a></p><a name="node"><font color="#404040">
<h2 id="_4">共享层</h2>
<p>另一个使用函数式模型的场合是使用共享层的时候。</p>
<p>考虑微博数据，我们希望建立模型来判别两条微博是否是来自同一个用户，这个需求同样可以用来判断一个用户的两条微博的相似性。</p>
<p>一种实现方式是，我们建立一个模型，它分别将两条微博的数据映射到两个特征向量上，然后将特征向量串联并加一个logistic回归层，输出它们来自同一个用户的概率。这种模型的训练数据是一对对的微博。</p>
<p>因为这个问题是对称的，所以处理第一条微博的模型当然也能重用于处理第二条微博。所以这里我们使用一个共享的LSTM层来进行映射。</p>
<p>首先，我们将微博的数据转为（140，256）的矩阵，即每条微博有140个字符，每个单词的特征由一个256维的词向量表示，向量的每个元素为1表示某个字符出现，为0表示不出现，这是一个one-hot编码。</p>
<p>之所以是（140，256）是因为一条微博最多有140个字符，而扩展的ASCII码表编码了常见的256个字符。原文中此处为Tweet，所以对外国人而言这是合理的。如果考虑中文字符，那一个单词的词向量就不止256了。</p>
<pre><code class="python hljs"><span class="hljs-keyword">import</span> keras
<span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> Input, LSTM, Dense
<span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> Model

tweet_a = Input(shape=(<span class="hljs-number">140</span>, <span class="hljs-number">256</span>))
tweet_b = Input(shape=(<span class="hljs-number">140</span>, <span class="hljs-number">256</span>))
</code></pre>

<p>若要对不同的输入共享同一层，就初始化该层一次，然后多次调用它</p>
<pre><code class="python hljs"><span class="hljs-comment"># This layer can take as input a matrix</span>
<span class="hljs-comment"># and will return a vector of size 64</span>
shared_lstm = LSTM(<span class="hljs-number">64</span>)

<span class="hljs-comment"># When we reuse the same layer instance</span>
<span class="hljs-comment"># multiple times, the weights of the layer</span>
<span class="hljs-comment"># are also being reused</span>
<span class="hljs-comment"># (it is effectively *the same* layer)</span>
encoded_a = shared_lstm(tweet_a)
encoded_b = shared_lstm(tweet_b)

<span class="hljs-comment"># We can then concatenate the two vectors:</span>
merged_vector = keras.layers.concatenate([encoded_a, encoded_b], axis=-<span class="hljs-number">1</span>)

<span class="hljs-comment"># And add a logistic regression on top</span>
predictions = Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">'sigmoid'</span>)(merged_vector)

<span class="hljs-comment"># We define a trainable model linking the</span>
<span class="hljs-comment"># tweet inputs to the predictions</span>
model = Model(inputs=[tweet_a, tweet_b], outputs=predictions)

model.compile(optimizer=<span class="hljs-string">'rmsprop'</span>,
              loss=<span class="hljs-string">'binary_crossentropy'</span>,
              metrics=[<span class="hljs-string">'accuracy'</span>])
model.fit([data_a, data_b], labels, epochs=<span class="hljs-number">10</span>)
</code></pre>

<p>先暂停一下，看看共享层到底输出了什么，它的输出数据shape又是什么</p>
<hr>
<h2 id="_5">层“节点”的概念</h2>
<p>无论何时，当你在某个输入上调用层时，你就创建了一个新的张量（即该层的输出），同时你也在为这个层增加一个“（计算）节点”。这个节点将输入张量映射为输出张量。当你多次调用该层时，这个层就有了多个节点，其下标分别为0，1，2...</p>
<p>在上一版本的Keras中，你可以通过<code>layer.get_output()</code>方法来获得层的输出张量，或者通过<code>layer.output_shape</code>获得其输出张量的shape。这个版本的Keras你仍然可以这么做（除了<code>layer.get_output()</code>被<code>output</code>替换）。但如果一个层与多个输入相连，会出现什么情况呢？</p>
<p>如果层只与一个输入相连，那没有任何困惑的地方。<code>.output</code>将会返回该层唯一的输出</p>
<pre><code class="python hljs">a = Input(shape=(<span class="hljs-number">140</span>, <span class="hljs-number">256</span>))

lstm = LSTM(<span class="hljs-number">32</span>)
encoded_a = lstm(a)

<span class="hljs-keyword">assert</span> lstm.output == encoded_a
</code></pre>

<p>但当层与多个输入相连时，会出现问题</p>
<pre><code class="hljs stylus"><span class="hljs-tag">a</span> = <span class="hljs-function"><span class="hljs-title">Input</span><span class="hljs-params">(shape=(<span class="hljs-number">140</span>, <span class="hljs-number">256</span>)</span></span>)
<span class="hljs-tag">b</span> = <span class="hljs-function"><span class="hljs-title">Input</span><span class="hljs-params">(shape=(<span class="hljs-number">140</span>, <span class="hljs-number">256</span>)</span></span>)

lstm = <span class="hljs-function"><span class="hljs-title">LSTM</span><span class="hljs-params">(<span class="hljs-number">32</span>)</span></span>
encoded_a = <span class="hljs-function"><span class="hljs-title">lstm</span><span class="hljs-params">(a)</span></span>
encoded_b = <span class="hljs-function"><span class="hljs-title">lstm</span><span class="hljs-params">(b)</span></span>

lstm<span class="hljs-class">.output</span>
</code></pre>

<p>上面这段代码会报错</p>
<pre><code class="python hljs">&gt;&gt; AssertionError: Layer lstm_1 has multiple inbound nodes,
hence the notion of <span class="hljs-string">"layer output"</span> <span class="hljs-keyword">is</span> ill-defined.
Use `get_output_at(node_index)` instead.
</code></pre>

<p>通过下面这种调用方式即可解决</p>
<pre><code class="python hljs"><span class="hljs-keyword">assert</span> lstm.get_output_at(<span class="hljs-number">0</span>) == encoded_a
<span class="hljs-keyword">assert</span> lstm.get_output_at(<span class="hljs-number">1</span>) == encoded_b
</code></pre>

</font></a><p><a name="node"><font color="#404040"></font>
</a></p>
<p>对于<code>input_shape</code>和<code>output_shape</code>也是一样，如果一个层只有一个节点，或所有的节点都有相同的输入或输出shape，那么<code>input_shape</code>和<code>output_shape</code>都是没有歧义的，并也只返回一个值。但是，例如你把一个相同的<code>Conv2D</code>应用于一个大小为(32,32,3)的数据，然后又将其应用于一个(64,64,3)的数据，那么此时该层就具有了多个输入和输出的shape，你就需要显式的指定节点的下标，来表明你想取的是哪个了</p>
<pre><code class="python hljs">a = Input(shape=(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">3</span>))
b = Input(shape=(<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, <span class="hljs-number">3</span>))

conv = Conv2D(<span class="hljs-number">16</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), padding=<span class="hljs-string">'same'</span>)
conved_a = conv(a)

<span class="hljs-comment"># Only one input so far, the following will work:</span>
<span class="hljs-keyword">assert</span> conv.input_shape == (<span class="hljs-keyword">None</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">3</span>)

conved_b = conv(b)
<span class="hljs-comment"># now the `.input_shape` property wouldn't work, but this does:</span>
<span class="hljs-keyword">assert</span> conv.get_input_shape_at(<span class="hljs-number">0</span>) == (<span class="hljs-keyword">None</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">3</span>)
<span class="hljs-keyword">assert</span> conv.get_input_shape_at(<span class="hljs-number">1</span>) == (<span class="hljs-keyword">None</span>, <span class="hljs-number">64</span>, <span class="hljs-number">64</span>, <span class="hljs-number">3</span>)
</code></pre>

<hr>
<h2 id="_6">更多的例子</h2>
<p>代码示例依然是学习的最佳方式，这里是更多的例子</p>
<h3 id="inception">inception模型</h3>
<p>inception的详细结构参见Google的这篇论文：<a href="http://arxiv.org/abs/1409.4842">Going Deeper with Convolutions</a></p>
<pre><code class="python hljs"><span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> Conv2D, MaxPooling2D, Input

input_img = Input(shape=(<span class="hljs-number">256</span>, <span class="hljs-number">256</span>, <span class="hljs-number">3</span>))

tower_1 = Conv2D(<span class="hljs-number">64</span>, (<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=<span class="hljs-string">'same'</span>, activation=<span class="hljs-string">'relu'</span>)(input_img)
tower_1 = Conv2D(<span class="hljs-number">64</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), padding=<span class="hljs-string">'same'</span>, activation=<span class="hljs-string">'relu'</span>)(tower_1)

tower_2 = Conv2D(<span class="hljs-number">64</span>, (<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=<span class="hljs-string">'same'</span>, activation=<span class="hljs-string">'relu'</span>)(input_img)
tower_2 = Conv2D(<span class="hljs-number">64</span>, (<span class="hljs-number">5</span>, <span class="hljs-number">5</span>), padding=<span class="hljs-string">'same'</span>, activation=<span class="hljs-string">'relu'</span>)(tower_2)

tower_3 = MaxPooling2D((<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), strides=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=<span class="hljs-string">'same'</span>)(input_img)
tower_3 = Conv2D(<span class="hljs-number">64</span>, (<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=<span class="hljs-string">'same'</span>, activation=<span class="hljs-string">'relu'</span>)(tower_3)

output = keras.layers.concatenate([tower_1, tower_2, tower_3], axis=<span class="hljs-number">1</span>)
</code></pre>

<h3 id="_7">卷积层的残差连接</h3>
<p>残差网络（Residual Network）的详细信息请参考这篇文章：<a href="http://arxiv.org/abs/1512.03385">Deep Residual Learning for Image Recognition</a></p>
<pre><code class="python hljs"><span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> Conv2D, Input

<span class="hljs-comment"># input tensor for a 3-channel 256x256 image</span>
x = Input(shape=(<span class="hljs-number">256</span>, <span class="hljs-number">256</span>, <span class="hljs-number">3</span>))
<span class="hljs-comment"># 3x3 conv with 3 output channels (same as input channels)</span>
y = Conv2D(<span class="hljs-number">3</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), padding=<span class="hljs-string">'same'</span>)(x)
<span class="hljs-comment"># this returns x + y.</span>
z = keras.layers.add([x, y])
</code></pre>

<h3 id="_8">共享视觉模型</h3>
<p>该模型在两个输入上重用了图像处理的模型，用来判别两个MNIST数字是否是相同的数字</p>
<pre><code class="python hljs"><span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> Conv2D, MaxPooling2D, Input, Dense, Flatten
<span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> Model

<span class="hljs-comment"># First, define the vision modules</span>
digit_input = Input(shape=(<span class="hljs-number">27</span>, <span class="hljs-number">27</span>, <span class="hljs-number">1</span>))
x = Conv2D(<span class="hljs-number">64</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>))(digit_input)
x = Conv2D(<span class="hljs-number">64</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>))(x)
x = MaxPooling2D((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>))(x)
out = Flatten()(x)

vision_model = Model(digit_input, out)

<span class="hljs-comment"># Then define the tell-digits-apart model</span>
digit_a = Input(shape=(<span class="hljs-number">27</span>, <span class="hljs-number">27</span>, <span class="hljs-number">1</span>))
digit_b = Input(shape=(<span class="hljs-number">27</span>, <span class="hljs-number">27</span>, <span class="hljs-number">1</span>))

<span class="hljs-comment"># The vision model will be shared, weights and all</span>
out_a = vision_model(digit_a)
out_b = vision_model(digit_b)

concatenated = keras.layers.concatenate([out_a, out_b])
out = Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">'sigmoid'</span>)(concatenated)

classification_model = Model([digit_a, digit_b], out)
</code></pre>

<h3 id="_9">视觉问答模型</h3>
<p>在针对一幅图片使用自然语言进行提问时，该模型能够提供关于该图片的一个单词的答案</p>
<p>这个模型将自然语言的问题和图片分别映射为特征向量，将二者合并后训练一个logistic回归层，从一系列可能的回答中挑选一个。</p>
<pre><code class="python hljs"><span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> Conv2D, MaxPooling2D, Flatten
<span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> Input, LSTM, Embedding, Dense
<span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> Model, Sequential

<span class="hljs-comment"># First, let's define a vision model using a Sequential model.</span>
<span class="hljs-comment"># This model will encode an image into a vector.</span>
vision_model = Sequential()
vision_model.add(Conv2D(<span class="hljs-number">64</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">'relu'</span>, padding=<span class="hljs-string">'same'</span>, input_shape=(<span class="hljs-number">224</span>, <span class="hljs-number">224</span>, <span class="hljs-number">3</span>)))
vision_model.add(Conv2D(<span class="hljs-number">64</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">'relu'</span>))
vision_model.add(MaxPooling2D((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)))
vision_model.add(Conv2D(<span class="hljs-number">128</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">'relu'</span>, padding=<span class="hljs-string">'same'</span>))
vision_model.add(Conv2D(<span class="hljs-number">128</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">'relu'</span>))
vision_model.add(MaxPooling2D((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)))
vision_model.add(Conv2D(<span class="hljs-number">256</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">'relu'</span>, padding=<span class="hljs-string">'same'</span>))
vision_model.add(Conv2D(<span class="hljs-number">256</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">'relu'</span>))
vision_model.add(Conv2D(<span class="hljs-number">256</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">'relu'</span>))
vision_model.add(MaxPooling2D((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)))
vision_model.add(Flatten())

<span class="hljs-comment"># Now let's get a tensor with the output of our vision model:</span>
image_input = Input(shape=(<span class="hljs-number">224</span>, <span class="hljs-number">224</span>, <span class="hljs-number">3</span>))
encoded_image = vision_model(image_input)

<span class="hljs-comment"># Next, let's define a language model to encode the question into a vector.</span>
<span class="hljs-comment"># Each question will be at most 100 word long,</span>
<span class="hljs-comment"># and we will index words as integers from 1 to 9999.</span>
question_input = Input(shape=(<span class="hljs-number">100</span>,), dtype=<span class="hljs-string">'int32'</span>)
embedded_question = Embedding(input_dim=<span class="hljs-number">10000</span>, output_dim=<span class="hljs-number">256</span>, input_length=<span class="hljs-number">100</span>)(question_input)
encoded_question = LSTM(<span class="hljs-number">256</span>)(embedded_question)

<span class="hljs-comment"># Let's concatenate the question vector and the image vector:</span>
merged = keras.layers.concatenate([encoded_question, encoded_image])

<span class="hljs-comment"># And let's train a logistic regression over 1000 words on top:</span>
output = Dense(<span class="hljs-number">1000</span>, activation=<span class="hljs-string">'softmax'</span>)(merged)

<span class="hljs-comment"># This is our final model:</span>
vqa_model = Model(inputs=[image_input, question_input], outputs=output)

<span class="hljs-comment"># The next stage would be training this model on actual data.</span>

</code></pre>

<h3 id="_10">视频问答模型</h3>
<p>在做完图片问答模型后，我们可以快速将其转为视频问答的模型。在适当的训练下，你可以为模型提供一个短视频（如100帧）然后向模型提问一个关于该视频的问题，如“what sport is the boy playing？”-&gt;“football”</p>
<pre><code class="python hljs"><span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> TimeDistributed

video_input = Input(shape=(<span class="hljs-number">100</span>, <span class="hljs-number">224</span>, <span class="hljs-number">224</span>, <span class="hljs-number">3</span>))
<span class="hljs-comment"># This is our video encoded via the previously trained vision_model (weights are reused)</span>
encoded_frame_sequence = TimeDistributed(vision_model)(video_input)  <span class="hljs-comment"># the output will be a sequence of vectors</span>
encoded_video = LSTM(<span class="hljs-number">256</span>)(encoded_frame_sequence)  <span class="hljs-comment"># the output will be a vector</span>

<span class="hljs-comment"># This is a model-level representation of the question encoder, reusing the same weights as before:</span>
question_encoder = Model(inputs=question_input, outputs=encoded_question)

<span class="hljs-comment"># Let's use it to encode the question:</span>
video_question_input = Input(shape=(<span class="hljs-number">100</span>,), dtype=<span class="hljs-string">'int32'</span>)
encoded_video_question = question_encoder(video_question_input)

<span class="hljs-comment"># And this is our video question answering model:</span>
merged = keras.layers.concatenate([encoded_video, encoded_video_question])
output = Dense(<span class="hljs-number">1000</span>, activation=<span class="hljs-string">'softmax'</span>)(merged)
video_qa_model = Model(inputs=[video_input, video_question_input], outputs=output)
</code></pre>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../../models/about_model/" class="btn btn-neutral float-right" title="关于Keras模型">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../sequential_model/" class="btn btn-neutral" title="序贯模型"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr><div><div id="rtd-qe50msmx" class="ethical-rtd"><div class="ethical-footer"><div class="ethical-content"><a href="https://readthedocs.org/sustainability/click/294/E6x5xU73X4Xj/" rel="nofollow" target="_blank" class="ethical-image-link"><img src="https://readthedocs.org/sustainability/view/294/E6x5xU73X4Xj/"></a><div class="ethical-text">A complete cloud platform designed for developers.<br><a href="https://readthedocs.org/sustainability/click/294/E6x5xU73X4Xj/" rel="nofollow" target="_blank">Try it free - $100 credit</a></div></div><div class="ethical-callout"><small><em><a href="https://readthedocs.org/sustainability/advertising/">Sponsored</a><span> · </span><a href="https://docs.readthedocs.io/en/latest/ethical-advertising.html">Ads served ethically</a></em></small></div></div></div><hr></div>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      <span class="fa fa-book"> Read the Docs</span>
      <span class="fa fa-caret-down"></span>
    </span>
    <div class="rst-other-versions"><!-- Inserted RTD Footer -->
<div class="injected">

  

      
      
      
      <dl>
        <dt>Versions</dt>
        
         <strong> 
        <dd><a href="https://keras-cn.readthedocs.io/en/latest/getting_started/functional_API/">latest</a></dd>
         </strong> 
        
      </dl>
      
      

      
      
      <dl>
        <dt>Downloads</dt>
        
        <dd><a href="//readthedocs.org/projects/keras-cn/downloads/pdf/latest/">PDF</a></dd>
        
        <dd><a href="//readthedocs.org/projects/keras-cn/downloads/htmlzip/latest/">HTML</a></dd>
        
        <dd><a href="//readthedocs.org/projects/keras-cn/downloads/epub/latest/">Epub</a></dd>
        
      </dl>
      
      

      
      <dl>
        <!-- These are kept as relative links for internal installs that are http -->
        <dt>On Read the Docs</dt>
        <dd>
          <a href="//readthedocs.org/projects/keras-cn/">Project Home</a>
        </dd>
        <dd>
          <a href="//readthedocs.org/projects/keras-cn/builds/">Builds</a>
        </dd>
        <dd>
          <a href="//readthedocs.org/projects/keras-cn/downloads/">Downloads</a>
        </dd>
      </dl>
      

      

      
      <dl>
        <dt>On GitHub</dt>
        <dd>
          <a href="https://github.com/MoyanZitto/keras-cn/blob/master/home/docs/checkouts/readthedocs.org/user_builds/keras-cn/checkouts/latest/docs/getting_started/functional_API.md">View</a>
        </dd>
        <dd>
          <a href="https://github.com/MoyanZitto/keras-cn/edit/master/home/docs/checkouts/readthedocs.org/user_builds/keras-cn/checkouts/latest/docs/getting_started/functional_API.md">Edit</a>
        </dd>
      </dl>
      
      

      
      <dl>
        <dt>Search</dt>
        <dd>
          <div style="padding: 6px;">
            <form id="flyout-search-form" class="wy-form" target="_blank" action="//readthedocs.org/projects/keras-cn/search/" method="get">
              <input type="text" name="q" placeholder="Search docs">
              </form>
          </div>
        </dd>
      </dl>
      



      <hr>
      
        <small>
          <span>Hosted by <a href="https://readthedocs.org">Read the Docs</a></span>
          <span> · </span>
          <a href="https://docs.readthedocs.io/en/latest/privacy-policy.html">Privacy Policy</a>
        </small>
      

      

</div>
</div>
  </div>



</body></html>