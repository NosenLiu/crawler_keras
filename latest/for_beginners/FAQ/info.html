<!DOCTYPE html><!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]--><!--[if gt IE 8]><!--><html class=" js flexbox flexboxlegacy canvas canvastext no-webgl touch no-geolocation postmessage websqldatabase indexeddb hashchange history draganddrop websockets rgba hsla multiplebgs backgroundsize borderimage borderradius boxshadow textshadow opacity cssanimations csscolumns cssgradients cssreflections csstransforms csstransforms3d csstransitions fontface generatedcontent no-video no-audio localstorage sessionstorage webworkers applicationcache svg inlinesvg smil svgclippaths" lang="en" style=""><!--<![endif]--><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <meta name="author" content="BigMoyan">
  
  <title>常见问题与解答 - Keras中文文档</title>
  

  <link rel="shortcut icon" href="../../img/favicon.ico">

  
  <link href="https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700" rel="stylesheet" type="text/css">

  <link rel="stylesheet" href="../../css/theme.css" type="text/css">
  <link rel="stylesheet" href="../../css/theme_extra.css" type="text/css">
  <link rel="stylesheet" href="../../css/highlight.css">
  <link href="https://media.readthedocs.org/css/badge_only.css" rel="stylesheet">
  <link href="https://media.readthedocs.org/css/readthedocs-doc-embed.css" rel="stylesheet">

  
  <script async="" src="https://www.google-analytics.com/analytics.js"></script><script>
    // Current page data
    var mkdocs_page_name = "常见问题与解答";
    var mkdocs_page_input_path = "for_beginners/FAQ.md";
    var mkdocs_page_url = "/for_beginners/FAQ/";
  </script>
  
  <script src="../../js/jquery-2.1.1.min.js"></script>
  <script src="../../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../../js/highlight.pack.js"></script>
  <script src="../../js/theme.js"></script> 
  <script src="../../readthedocs-data.js"></script>
  <script src="https://media.readthedocs.org/static/core/js/readthedocs-doc-embed.js"></script>
  <script src="https://media.readthedocs.org/javascript/readthedocs-analytics.js"></script>

  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../.." class="icon icon-home"> Keras中文文档</a>
        <div role="search">
  <form id="mkdocs-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs">
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        <ul class="current">
          
            <li>
    </li><li class="toctree-l1 ">
        <a class="" href="./../..info.html">主页</a>
        
    </li>
<li>
          
            </li><li>
    <ul class="subnav">
    <li><span>keras新手指南</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../concepts/info.html">一些基本概念</a>
        
    </li>

        
            
    <li class="toctree-l1 current">
        <a class="current" href="././info.html">常见问题与解答</a>
        
            <ul>
            
                <li class="toctree-l3"><a href="#keras-faq">Keras FAQ：常见问题</a></li>
                
                    <li><a class="toctree-l4" href="#keras">如何引用Keras？</a></li>
                
                    <li><a class="toctree-l4" href="#kerasgpu">如何使Keras调用GPU？</a></li>
                
                    <li><a class="toctree-l4" href="#gpukeras">如何在多张GPU卡上使用Keras？</a></li>
                
                    <li><a class="toctree-l4" href="#batch-epochsample">"batch", "epoch"和"sample"都是啥意思？？</a></li>
                
                    <li><a class="toctree-l4" href="#keras_1">如何保存Keras模型？</a></li>
                
                    <li><a class="toctree-l4" href="#_3">为什么训练误差比测试误差高很多？</a></li>
                
                    <li><a class="toctree-l4" href="#_4">如何获取中间层的输出？</a></li>
                
                    <li><a class="toctree-l4" href="#keras_2">如何利用Keras处理超过机器内存的数据集？</a></li>
                
                    <li><a class="toctree-l4" href="#loss">当验证集的loss不再下降时，如何中断训练？</a></li>
                
                    <li><a class="toctree-l4" href="#_5">验证集是如何从训练集中分割出来的？</a></li>
                
                    <li><a class="toctree-l4" href="#_6">训练数据在训练时会被随机洗乱吗？</a></li>
                
                    <li><a class="toctree-l4" href="#epochloss">如何在每个epoch后记录训练/测试的loss和正确率？</a></li>
                
                    <li><a class="toctree-l4" href="#rnnstateful-rnn">如何使用状态RNN（stateful RNN）？</a></li>
                
                    <li><a class="toctree-l4" href="#_7">如何“冻结”网络的层？</a></li>
                
                    <li><a class="toctree-l4" href="#sequential">如何从Sequential模型中去除一个层？</a></li>
                
                    <li><a class="toctree-l4" href="#keras_3">如何在Keras中使用预训练的模型？</a></li>
                
                    <li><a class="toctree-l4" href="#kerashdf5">如何在Keras中使用HDF5输入？</a></li>
                
                    <li><a class="toctree-l4" href="#keras_4">Keras的配置文件存储在哪里？</a></li>
                
                    <li><a class="toctree-l4" href="#keras_5">在使用Keras开发过程中，我如何获得可复现的结果?</a></li>
                
            
            </ul>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../keras_linux/info.html">Keras安装和配置指南(Linux)</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../keras_windows/info.html">Keras安装和配置指南(Windows)</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../trap/info.html">Keras使用陷阱</a>
        
    </li>

        
    </ul>
</li><li>
          
            </li><li>
    <ul class="subnav">
    <li><span>快速开始</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../getting_started/sequential_model/info.html">序贯模型</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../getting_started/functional_API/info.html">函数式模型</a>
        
    </li>

        
    </ul>
</li><li>
          
            </li><li>
    <ul class="subnav">
    <li><span>模型</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../models/about_model/info.html">关于Keras模型</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../models/sequential/info.html">序贯模型API</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../models/model/info.html">函数式模型API</a>
        
    </li>

        
    </ul>
</li><li>
          
            </li><li>
    <ul class="subnav">
    <li><span>网络层</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../layers/about_layer/info.html">关于Keras层</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../layers/core_layer/info.html">常用层Core</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../layers/convolutional_layer/info.html">卷积层Convolutional</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../layers/pooling_layer/info.html">池化层Pooling</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../layers/locally_connected_layer/info.html">局部连接层Locally-connented</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../layers/recurrent_layer/info.html">循环层Recurrent</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../layers/embedding_layer/info.html">嵌入层Embedding</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../layers/merge/info.html">融合层Merge</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../layers/advanced_activation_layer/info.html">高级激活层Advanced Activation</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../layers/normalization_layer/info.html">规范层BatchNormalization</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../layers/noise_layer/info.html">噪声层Noise</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../layers/wrapper/info.html">包装器Wrapper</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../layers/writting_layer/info.html">编写自己的层</a>
        
    </li>

        
    </ul>
</li><li>
          
            </li><li>
    <ul class="subnav">
    <li><span>数据预处理</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../preprocessing/sequence/info.html">序列预处理</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../preprocessing/text/info.html">文本预处理</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../preprocessing/image/info.html">图片预处理</a>
        
    </li>

        
    </ul>
</li><li>
          
            </li><li>
    <ul class="subnav">
    <li><span>网络配置</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../other/objectives/info.html">损失函数loss</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../other/optimizers/info.html">优化器Optimizer</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../other/activations/info.html">激活函数Activation</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../other/metrics/info.html">性能评估Metrices</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../other/initializations/info.html">初始化方法Initializers</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../other/regularizers/info.html">正则项Regularizer</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../other/constraints/info.html">约束项Constraint</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../other/callbacks/info.html">回调函数Callback</a>
        
    </li>

        
    </ul>
</li><li>
          
            </li><li>
    <ul class="subnav">
    <li><span>协助使用Keras</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../other/application/info.html">预训练模型Application</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../other/datasets/info.html">常用数据库Dataset</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../other/visualization/info.html">可视化visualization</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="./../../utils/info.html">工具</a>
        
    </li>

        
    </ul>
</li><li>
          
            </li><li>
    </li><li class="toctree-l1 ">
        <a class="" href="./../../backend/info.html">keras后端Backend</a>
        
    </li>
<li>
          
            </li><li>
    </li><li class="toctree-l1 ">
        <a class="" href="./../../scikit-learn_API/info.html">scikit-learn接口</a>
        
    </li>
<li>
          
            </li><li>
    </li><li class="toctree-l1 ">
        <a class="" href="./../../acknowledgement/info.html">致谢</a>
        
    </li>
<li>
          
        </li></ul>
      </div>
      &nbsp;
    <div id="rtd-hwahtegr" class="ethical-rtd"></div></nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../..">Keras中文文档</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../..">Docs</a> »</li>
    
      
        
          <li>keras新手指南 »</li>
        
      
    
    <li>常见问题与解答</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="keras-faq">Keras FAQ：常见问题</h1>
<ul>
<li><a href="#citation">如何引用Keras？</a></li>
<li><a href="#GPU">如何使Keras调用GPU？</a></li>
<li><a href="#multi-GPU">如何在多张GPU卡上使用Keras</a></li>
<li><a href="#batch">"batch", "epoch"和"sample"都是啥意思？</a></li>
<li><a href="#save_model">如何保存Keras模型？</a></li>
<li><a href="#loss">为什么训练误差(loss)比测试误差高很多？</a></li>
<li><a href="#intermediate_layer">如何获取中间层的输出？</a></li>
<li><a href="#dataset">如何利用Keras处理超过机器内存的数据集？</a></li>
<li><a href="#stop_train">当验证集的loss不再下降时，如何中断训练？</a></li>
<li><a href="#validation_spilt">验证集是如何从训练集中分割出来的？</a></li>
<li><a href="#shuffle">训练数据在训练时会被随机洗乱吗？</a></li>
<li><a href="#history">如何在每个epoch后记录训练/测试的loss和正确率？</a></li>
<li><a href="#stateful_RNN">如何使用状态RNN（stateful RNN）？</a></li>
<li><a href="#freeze">如何“冻结”网络的层？</a></li>
<li><a href="#pop">如何从Sequential模型中去除一个层？</a></li>
<li><a href="#pretrain">如何在Keras中使用预训练的模型</a></li>
<li><a href="#hdf5">如何在Keras中使用HDF5输入？</a></li>
<li><a href="#where_config">Keras的配置文件存储在哪里？</a></li>
<li><a href="#reproduce">在使用Keras开发过程中，我如何获得可复现的结果？</a></li>
</ul>
<hr>
<p><a name="citation">
<font color="#404040"></font></a></p><a name="citation"><font color="#404040">
<h2 id="keras">如何引用Keras？</h2>
</font></a><p><a name="citation"><font color="#404040"></font>
</a></p>
<p>如果Keras对你的研究有帮助的话，请在你的文章中引用Keras。这里是一个使用BibTex的例子</p>
<pre><code class="python hljs"><span class="hljs-decorator">@misc{chollet2015keras,</span>
  author = {Chollet, François <span class="hljs-keyword">and</span> others},
  title = {Keras},
  year = {<span class="hljs-number">2015</span>},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/fchollet/keras}}
}
</code></pre>

<hr>
<p><a name="GPU">
<font color="#404040"></font></a></p><a name="GPU"><font color="#404040">
<h2 id="kerasgpu">如何使Keras调用GPU？</h2>
</font></a><p><a name="GPU"><font color="#404040"></font>
</a></p>
<p>如果采用TensorFlow作为后端，当机器上有可用的GPU时，代码会自动调用GPU进行并行计算。如果使用Theano作为后端，可以通过以下方法设置：</p>
<p>方法1：使用Theano标记</p>
<p>在执行python脚本时使用下面的命令：</p>
<pre><code class="python hljs">THEANO_FLAGS=device=gpu,floatX=float32 python my_keras_script.py
</code></pre>

<p>方法2：设置<code>.theano</code>文件</p>
<p>点击<a href="http://deeplearning.net/software/theano/library/config.html">这里</a>查看指导教程</p>
<p>方法3：在代码的开头处手动设置<code>theano.config.device</code>和<code>theano.config.floatX</code></p>
<pre><code class="python hljs">    <span class="hljs-keyword">import</span> theano
    theano.config.device = <span class="hljs-string">'gpu'</span>
    theano.config.floatX = <span class="hljs-string">'float32'</span>
</code></pre>

<p><a name="multi-GPU">
<font color="#404040"></font></a></p><a name="multi-GPU"><font color="#404040">
<h2 id="gpukeras">如何在多张GPU卡上使用Keras？</h2>
</font></a><p><a name="multi-GPU"><font color="#404040"></font>
</a></p>
<p>我们建议有多张GPU卡可用时，使用TnesorFlow后端。</p>
<p>有两种方法可以在多张GPU上运行一个模型：数据并行/设备并行</p>
<p>大多数情况下，你需要的很可能是“数据并行”</p>
<h3 id="_1">数据并行</h3>
<p>数据并行将目标模型在多个设备上各复制一份，并使用每个设备上的复制品处理整个数据集的不同部分数据。Keras在<code>keras.utils.multi_gpu_model</code>中提供有内置函数，该函数可以产生任意模型的数据并行版本，最高支持在8片GPU上并行。
请参考<a href="../../utils/">utils</a>中的multi_gpu_model文档。 下面是一个例子：</p>
<pre><code class="python hljs"><span class="hljs-keyword">from</span> keras.utils <span class="hljs-keyword">import</span> multi_gpu_model

<span class="hljs-comment"># Replicates `model` on 8 GPUs.</span>
<span class="hljs-comment"># This assumes that your machine has 8 available GPUs.</span>
parallel_model = multi_gpu_model(model, gpus=<span class="hljs-number">8</span>)
parallel_model.compile(loss=<span class="hljs-string">'categorical_crossentropy'</span>,
                       optimizer=<span class="hljs-string">'rmsprop'</span>)

<span class="hljs-comment"># This `fit` call will be distributed on 8 GPUs.</span>
<span class="hljs-comment"># Since the batch size is 256, each GPU will process 32 samples.</span>
parallel_model.fit(x, y, epochs=<span class="hljs-number">20</span>, batch_size=<span class="hljs-number">256</span>)
</code></pre>

<h3 id="_2">设备并行</h3>
<p>设备并行是在不同设备上运行同一个模型的不同部分，当模型含有多个并行结构，例如含有两个分支时，这种方式很适合。</p>
<p>这种并行方法可以通过使用TensorFlow device scopes实现，下面是一个例子：</p>
<pre><code class="python hljs"><span class="hljs-comment"># Model where a shared LSTM is used to encode two different sequences in parallel</span>
input_a = keras.Input(shape=(<span class="hljs-number">140</span>, <span class="hljs-number">256</span>))
input_b = keras.Input(shape=(<span class="hljs-number">140</span>, <span class="hljs-number">256</span>))

shared_lstm = keras.layers.LSTM(<span class="hljs-number">64</span>)

<span class="hljs-comment"># Process the first sequence on one GPU</span>
<span class="hljs-keyword">with</span> tf.device_scope(<span class="hljs-string">'/gpu:0'</span>):
    encoded_a = shared_lstm(tweet_a)
<span class="hljs-comment"># Process the next sequence on another GPU</span>
<span class="hljs-keyword">with</span> tf.device_scope(<span class="hljs-string">'/gpu:1'</span>):
    encoded_b = shared_lstm(tweet_b)

<span class="hljs-comment"># Concatenate results on CPU</span>
<span class="hljs-keyword">with</span> tf.device_scope(<span class="hljs-string">'/cpu:0'</span>):
    merged_vector = keras.layers.concatenate([encoded_a, encoded_b],
                                             axis=-<span class="hljs-number">1</span>)
</code></pre>

<hr>
<p><a name="batch">
<font color="#404040"></font></a></p><a name="batch"><font color="#404040">
<h2 id="batch-epochsample">"batch", "epoch"和"sample"都是啥意思？？</h2>
</font></a><p><a name="batch"><font color="#404040"></font>
</a></p>
<p>下面是一些使用keras时常会遇到的概念，我们来简单解释。</p>
<ul>
<li>Sample：样本，数据集中的一条数据。例如图片数据集中的一张图片，语音数据中的一段音频。</li>
<li>Batch：中文为批，一个batch由若干条数据构成。batch是进行网络优化的基本单位，网络参数的每一轮优化需要使用一个batch。batch中的样本是被并行处理的。与单个样本相比，一个batch的数据能更好的模拟数据集的分布，batch越大则对输入数据分布模拟的越好，反应在网络训练上，则体现为能让网络训练的方向“更加正确”。但另一方面，一个batch也只能让网络的参数更新一次，因此网络参数的迭代会较慢。在测试网络的时候，应该在条件的允许的范围内尽量使用更大的batch，这样计算效率会更高。</li>
<li>Epoch，epoch可译为“轮次”。如果说每个batch对应网络的一次更新的话，一个epoch对应的就是网络的一轮更新。每一轮更新中网络更新的次数可以随意，但通常会设置为遍历一遍数据集。因此一个epoch的含义是模型完整的看了一遍数据集。
    设置epoch的主要作用是把模型的训练的整个训练过程分为若干个段，这样我们可以更好的观察和调整模型的训练。Keras中，当指定了验证集时，每个epoch执行完后都会运行一次验证集以确定模型的性能。另外，我们可以使用回调函数在每个epoch的训练前后执行一些操作，如调整学习率，打印目前模型的一些信息等，详情请参考Callback一节。</li>
</ul>
<hr>
<p><a name="save_model">
<font color="#404040"></font></a></p><a name="save_model"><font color="#404040">
<h2 id="keras_1">如何保存Keras模型？</h2>
</font></a><p><a name="save_model"><font color="#404040"></font>
</a></p>
<p>我们不推荐使用pickle或cPickle来保存Keras模型</p>
<p>你可以使用<code>model.save(filepath)</code>将Keras模型和权重保存在一个HDF5文件中，该文件将包含：</p>
<ul>
<li>模型的结构，以便重构该模型</li>
<li>模型的权重</li>
<li>训练配置（损失函数，优化器等）</li>
<li>优化器的状态，以便于从上次训练中断的地方开始</li>
</ul>
<p>使用<code>keras.models.load_model(filepath)</code>来重新实例化你的模型，如果文件中存储了训练配置的话，该函数还会同时完成模型的编译</p>
<p>例子：</p>
<pre><code class="python hljs"><span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> load_model

model.save(<span class="hljs-string">'my_model.h5'</span>)  <span class="hljs-comment"># creates a HDF5 file 'my_model.h5'</span>
<span class="hljs-keyword">del</span> model  <span class="hljs-comment"># deletes the existing model</span>

<span class="hljs-comment"># returns a compiled model</span>
<span class="hljs-comment"># identical to the previous one</span>
model = load_model(<span class="hljs-string">'my_model.h5'</span>)
</code></pre>

<p>如果你只是希望保存模型的结构，而不包含其权重或配置信息，可以使用：</p>
<pre><code class="python hljs"><span class="hljs-comment"># save as JSON</span>
json_string = model.to_json()

<span class="hljs-comment"># save as YAML</span>
yaml_string = model.to_yaml()
</code></pre>

<p>这项操作将把模型序列化为json或yaml文件，这些文件对人而言也是友好的，如果需要的话你甚至可以手动打开这些文件并进行编辑。</p>
<p>当然，你也可以从保存好的json文件或yaml文件中载入模型：</p>
<pre><code class="python hljs"><span class="hljs-comment"># model reconstruction from JSON:</span>
<span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> model_from_json
model = model_from_json(json_string)

<span class="hljs-comment"># model reconstruction from YAML</span>
model = model_from_yaml(yaml_string)
</code></pre>

<p>如果需要保存模型的权重，可通过下面的代码利用HDF5进行保存。注意，在使用前需要确保你已安装了HDF5和其Python库h5py</p>
<pre><code class="hljs stylus">model.<span class="hljs-function"><span class="hljs-title">save_weights</span><span class="hljs-params">(<span class="hljs-string">'my_model_weights.h5'</span>)</span></span>
</code></pre>

<p>如果你需要在代码中初始化一个完全相同的模型，请使用：</p>
<pre><code class="python hljs">model.load_weights(<span class="hljs-string">'my_model_weights.h5'</span>)
</code></pre>

<p>如果你需要加载权重到不同的网络结构（有些层一样）中，例如fine-tune或transfer-learning，你可以通过层名字来加载模型：</p>
<pre><code class="python hljs">model.load_weights(<span class="hljs-string">'my_model_weights.h5'</span>, by_name=<span class="hljs-keyword">True</span>)
</code></pre>

<p>例如：</p>
<pre><code class="python hljs"><span class="hljs-string">"""
假如原模型为：
    model = Sequential()
    model.add(Dense(2, input_dim=3, name="dense_1"))
    model.add(Dense(3, name="dense_2"))
    ...
    model.save_weights(fname)
"""</span>
<span class="hljs-comment"># new model</span>
model = Sequential()
model.add(Dense(<span class="hljs-number">2</span>, input_dim=<span class="hljs-number">3</span>, name=<span class="hljs-string">"dense_1"</span>))  <span class="hljs-comment"># will be loaded</span>
model.add(Dense(<span class="hljs-number">10</span>, name=<span class="hljs-string">"new_dense"</span>))  <span class="hljs-comment"># will not be loaded</span>

<span class="hljs-comment"># load weights from first model; will only affect the first layer, dense_1.</span>
model.load_weights(fname, by_name=<span class="hljs-keyword">True</span>)

</code></pre>

<hr>
<p><a name="loss">
<font color="#404040"></font></a></p><a name="loss"><font color="#404040">
<h2 id="_3">为什么训练误差比测试误差高很多？</h2>
</font></a><p><a name="loss"><font color="#404040"></font>
</a></p>
<p>一个Keras的模型有两个模式：训练模式和测试模式。一些正则机制，如Dropout，L1/L2正则项在测试模式下将不被启用。</p>
<p>另外，训练误差是训练数据每个batch的误差的平均。在训练过程中，每个epoch起始时的batch的误差要大一些，而后面的batch的误差要小一些。另一方面，每个epoch结束时计算的测试误差是由模型在epoch结束时的状态决定的，这时候的网络将产生较小的误差。</p>
<p>【Tips】可以通过定义回调函数将每个epoch的训练误差和测试误差并作图，如果训练误差曲线和测试误差曲线之间有很大的空隙，说明你的模型可能有过拟合的问题。当然，这个问题与Keras无关。</p>
<hr>
<p><a name="intermediate_layer">
<font color="#404040"></font></a></p><a name="intermediate_layer"><font color="#404040">
<h2 id="_4">如何获取中间层的输出？</h2>
</font></a><p><a name="intermediate_layer"><font color="#404040"></font>
</a></p>
<p>一种简单的方法是创建一个新的<code>Model</code>，使得它的输出是你想要的那个输出</p>
<pre><code class="python hljs"><span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> Model

model = ...  <span class="hljs-comment"># create the original model</span>

layer_name = <span class="hljs-string">'my_layer'</span>
intermediate_layer_model = Model(input=model.input,
                                 output=model.get_layer(layer_name).output)
intermediate_output = intermediate_layer_model.predict(data
</code></pre>

<p>此外，我们也可以建立一个Keras的函数来达到这一目的：</p>
<pre><code class="python hljs"><span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> backend <span class="hljs-keyword">as</span> K

<span class="hljs-comment"># with a Sequential model</span>
get_3rd_layer_output = K.function([model.layers[<span class="hljs-number">0</span>].input],
                                  [model.layers[<span class="hljs-number">3</span>].output])
layer_output = get_3rd_layer_output([X])[<span class="hljs-number">0</span>]
</code></pre>

<p>当然，我们也可以直接编写Theano和TensorFlow的函数来完成这件事</p>
<p>注意，如果你的模型在训练和测试两种模式下不完全一致，例如你的模型中含有Dropout层，批规范化（BatchNormalization）层等组件，你需要在函数中传递一个learning_phase的标记，像这样：</p>
<pre><code class="hljs prolog"><span class="hljs-atom">get_3rd_layer_output</span> = <span class="hljs-name">K</span>.<span class="hljs-atom">function</span>([<span class="hljs-atom">model</span>.<span class="hljs-atom">layers</span>[<span class="hljs-number">0</span>].<span class="hljs-atom">input</span>, <span class="hljs-name">K</span>.<span class="hljs-atom">learning_phase</span>()],
                                  [<span class="hljs-atom">model</span>.<span class="hljs-atom">layers</span>[<span class="hljs-number">3</span>].<span class="hljs-atom">output</span>])

# <span class="hljs-atom">output</span> <span class="hljs-atom">in</span> <span class="hljs-atom">test</span> <span class="hljs-atom">mode</span> = <span class="hljs-number">0</span>
<span class="hljs-atom">layer_output</span> = <span class="hljs-atom">get_3rd_layer_output</span>([<span class="hljs-name">X</span>, <span class="hljs-number">0</span>])[<span class="hljs-number">0</span>]

# <span class="hljs-atom">output</span> <span class="hljs-atom">in</span> <span class="hljs-atom">train</span> <span class="hljs-atom">mode</span> = <span class="hljs-number">1</span>
<span class="hljs-atom">layer_output</span> = <span class="hljs-atom">get_3rd_layer_output</span>([<span class="hljs-name">X</span>, <span class="hljs-number">1</span>])[<span class="hljs-number">0</span>]
</code></pre>

<hr>
<p><a name="dataset">
<font color="#404040"></font></a></p><a name="dataset"><font color="#404040">
<h2 id="keras_2">如何利用Keras处理超过机器内存的数据集？</h2>
</font></a><p><a name="dataset"><font color="#404040"></font>
</a></p>
<p>可以使用<code>model.train_on_batch(X,y)</code>和<code>model.test_on_batch(X,y)</code>。请参考<a href="../../models/sequential/">模型</a></p>
<p>另外，也可以编写一个每次产生一个batch样本的生成器函数，并调用<code>model.fit_generator(data_generator, samples_per_epoch, nb_epoch)</code>进行训练</p>
<p>这种方式在Keras代码包的example文件夹下CIFAR10例子里有示范，也可点击<a href="https://github.com/fchollet/keras/blob/master/examples/cifar10_cnn.py">这里</a>在github上浏览。</p>
<hr>
<p><a name="early_stopping">
<font color="#404040"></font></a></p><a name="early_stopping"><font color="#404040">
<h2 id="loss">当验证集的loss不再下降时，如何中断训练？</h2>
</font></a><p><a name="early_stopping"><font color="#404040"></font>
</a></p>
<p>可以定义<code>EarlyStopping</code>来提前终止训练</p>
<pre><code class="python hljs"><span class="hljs-keyword">from</span> keras.callbacks <span class="hljs-keyword">import</span> EarlyStopping
early_stopping = EarlyStopping(monitor=<span class="hljs-string">'val_loss'</span>, patience=<span class="hljs-number">2</span>)
model.fit(X, y, validation_split=<span class="hljs-number">0.2</span>, callbacks=[early_stopping])
</code></pre>

<p>请参考<a href="../../other/callbacks">回调函数</a></p>
<hr>
<p><a name="validation_spilt">
<font color="#404040"></font></a></p><a name="validation_spilt"><font color="#404040">
<h2 id="_5">验证集是如何从训练集中分割出来的？</h2>
</font></a><p><a name="validation_spilt"><font color="#404040"></font>
</a></p>
<p>如果在<code>model.fit</code>中设置<code>validation_spilt</code>的值，则可将数据分为训练集和验证集，例如，设置该值为0.1，则训练集的最后10%数据将作为验证集，设置其他数字同理。注意，原数据在进行验证集分割前并没有被shuffle，所以这里的验证集严格的就是你输入数据最末的x%。</p>
<hr>
<p><a name="shuffle">
<font color="#404040"></font></a></p><a name="shuffle"><font color="#404040">
<h2 id="_6">训练数据在训练时会被随机洗乱吗？</h2>
</font></a><p><a name="shuffle"><font color="#404040"></font>
</a></p>
<p>是的，如果<code>model.fit</code>的<code>shuffle</code>参数为真，训练的数据就会被随机洗乱。不设置时默认为真。训练数据会在每个epoch的训练中都重新洗乱一次。</p>
<p>验证集的数据不会被洗乱</p>
<hr>
<p><a name="history">
<font color="#404040"></font></a></p><a name="history"><font color="#404040">
<h2 id="epochloss">如何在每个epoch后记录训练/测试的loss和正确率？</h2>
</font></a><p><a name="history"><font color="#404040"></font>
</a></p>
<p><code>model.fit</code>在运行结束后返回一个<code>History</code>对象，其中含有的<code>history</code>属性包含了训练过程中损失函数的值以及其他度量指标。</p>
<pre><code class="python hljs">hist = model.fit(X, y, validation_split=<span class="hljs-number">0.2</span>)
print(hist.history)
</code></pre>

<hr>
<p><a name="stateful_RNN">
<font color="#404040"></font></a></p><a name="stateful_RNN"><font color="#404040">
<h2 id="rnnstateful-rnn">如何使用状态RNN（stateful RNN）？</h2>
</font></a><p><a name="stateful_RNN"><font color="#404040"></font>
</a></p>
<p>一个RNN是状态RNN，意味着训练时每个batch的状态都会被重用于初始化下一个batch的初始状态。</p>
<p>当使用状态RNN时，有如下假设</p>
<ul>
<li>
<p>所有的batch都具有相同数目的样本</p>
</li>
<li>
<p>如果<code>X1</code>和<code>X2</code>是两个相邻的batch，那么对于任何<code>i</code>，<code>X2[i]</code>都是<code>X1[i]</code>的后续序列</p>
</li>
</ul>
<p>要使用状态RNN，我们需要</p>
<ul>
<li>
<p>显式的指定每个batch的大小。可以通过模型的首层参数<code>batch_input_shape</code>来完成。<code>batch_input_shape</code>是一个整数tuple，例如(32,10,16)代表一个具有10个时间步，每步向量长为16，每32个样本构成一个batch的输入数据格式。</p>
</li>
<li>
<p>在RNN层中，设置<code>stateful=True</code></p>
</li>
</ul>
<p>要重置网络的状态，使用：</p>
<ul>
<li>
<p><code>model.reset_states()</code>来重置网络中所有层的状态</p>
</li>
<li>
<p><code>layer.reset_states()</code>来重置指定层的状态</p>
</li>
</ul>
<p>例子：</p>
<pre><code class="python hljs">X  <span class="hljs-comment"># this is our input data, of shape (32, 21, 16)</span>
<span class="hljs-comment"># we will feed it to our model in sequences of length 10</span>

model = Sequential()
model.add(LSTM(<span class="hljs-number">32</span>, input_shape=(<span class="hljs-number">10</span>, <span class="hljs-number">16</span>), batch_size=<span class="hljs-number">32</span>, stateful=<span class="hljs-keyword">True</span>))
model.add(Dense(<span class="hljs-number">16</span>, activation=<span class="hljs-string">'softmax'</span>))

model.compile(optimizer=<span class="hljs-string">'rmsprop'</span>, loss=<span class="hljs-string">'categorical_crossentropy'</span>)

<span class="hljs-comment"># we train the network to predict the 11th timestep given the first 10:</span>
model.train_on_batch(X[:, :<span class="hljs-number">10</span>, :], np.reshape(X[:, <span class="hljs-number">10</span>, :], (<span class="hljs-number">32</span>, <span class="hljs-number">16</span>)))

<span class="hljs-comment"># the state of the network has changed. We can feed the follow-up sequences:</span>
model.train_on_batch(X[:, <span class="hljs-number">10</span>:<span class="hljs-number">20</span>, :], np.reshape(X[:, <span class="hljs-number">20</span>, :], (<span class="hljs-number">32</span>, <span class="hljs-number">16</span>)))

<span class="hljs-comment"># let's reset the states of the LSTM layer:</span>
model.reset_states()

<span class="hljs-comment"># another way to do it in this case:</span>
model.layers[<span class="hljs-number">0</span>].reset_states()
</code></pre>

<p>注意，<code>predict</code>，<code>fit</code>，<code>train_on_batch</code>
，<code>predict_classes</code>等方法都会更新模型中状态层的状态。这使得你不但可以进行状态网络的训练，也可以进行状态网络的预测。</p>
<hr>
<p><a name="freeze">
<font color="#404040"></font></a></p><a name="freeze"><font color="#404040">
<h2 id="_7">如何“冻结”网络的层？</h2>
</font></a><p><a name="freeze"><font color="#404040"></font>
</a></p>
<p>“冻结”一个层指的是该层将不参加网络训练，即该层的权重永不会更新。在进行fine-tune时我们经常会需要这项操作。
在使用固定的embedding层处理文本输入时，也需要这个技术。</p>
<p>可以通过向层的构造函数传递<code>trainable</code>参数来指定一个层是不是可训练的，如：</p>
<pre><code class="python hljs">frozen_layer = Dense(<span class="hljs-number">32</span>,trainable=<span class="hljs-keyword">False</span>)
</code></pre>

<p>此外，也可以通过将层对象的<code>trainable</code>属性设为<code>True</code>或<code>False</code>来为已经搭建好的模型设置要冻结的层。
在设置完后，需要运行<code>compile</code>来使设置生效，例如：</p>
<pre><code class="python hljs">x = Input(shape=(<span class="hljs-number">32</span>,))
layer = Dense(<span class="hljs-number">32</span>)
layer.trainable = <span class="hljs-keyword">False</span>
y = layer(x)

frozen_model = Model(x, y)
<span class="hljs-comment"># in the model below, the weights of `layer` will not be updated during training</span>
frozen_model.compile(optimizer=<span class="hljs-string">'rmsprop'</span>, loss=<span class="hljs-string">'mse'</span>)

layer.trainable = <span class="hljs-keyword">True</span>
trainable_model = Model(x, y)
<span class="hljs-comment"># with this model the weights of the layer will be updated during training</span>
<span class="hljs-comment"># (which will also affect the above model since it uses the same layer instance)</span>
trainable_model.compile(optimizer=<span class="hljs-string">'rmsprop'</span>, loss=<span class="hljs-string">'mse'</span>)

frozen_model.fit(data, labels)  <span class="hljs-comment"># this does NOT update the weights of `layer`</span>
trainable_model.fit(data, labels)  <span class="hljs-comment"># this updates the weights of `layer`</span>
</code></pre>

<hr>
<p><a name="pop">
<font color="#404040"></font></a></p><a name="pop"><font color="#404040">
<h2 id="sequential">如何从Sequential模型中去除一个层？</h2>
</font></a><p><a name="pop"><font color="#404040"></font>
</a></p>
<p>可以通过调用<code>.pop()</code>来去除模型的最后一个层，反复调用n次即可去除模型后面的n个层</p>
<pre><code class="python hljs">model = Sequential()
model.add(Dense(<span class="hljs-number">32</span>, activation=<span class="hljs-string">'relu'</span>, input_dim=<span class="hljs-number">784</span>))
model.add(Dense(<span class="hljs-number">32</span>, activation=<span class="hljs-string">'relu'</span>))

print(len(model.layers))  <span class="hljs-comment"># "2"</span>

model.pop()
print(len(model.layers))  <span class="hljs-comment"># "1"</span>
</code></pre>

<hr>
<p><a name="pretrain">
<font color="#404040"></font></a></p><a name="pretrain"><font color="#404040">
<h2 id="keras_3">如何在Keras中使用预训练的模型？</h2>
</font></a><p><a name="pretrain"><font color="#404040"></font>
</a></p>
<p>我们提供了下面这些图像分类的模型代码及预训练权重：</p>
<ul>
<li>VGG16</li>
<li>VGG19</li>
<li>ResNet50</li>
<li>Inception v3</li>
</ul>
<p>可通过<code>keras.applications</code>载入这些模型：</p>
<pre><code class="python hljs"><span class="hljs-keyword">from</span> keras.applications.vgg16 <span class="hljs-keyword">import</span> VGG16
<span class="hljs-keyword">from</span> keras.applications.vgg19 <span class="hljs-keyword">import</span> VGG19
<span class="hljs-keyword">from</span> keras.applications.resnet50 <span class="hljs-keyword">import</span> ResNet50
<span class="hljs-keyword">from</span> keras.applications.inception_v3 <span class="hljs-keyword">import</span> InceptionV3

model = VGG16(weights=<span class="hljs-string">'imagenet'</span>, include_top=<span class="hljs-keyword">True</span>)
</code></pre>

<p>这些代码的使用示例请参考<code>.Application</code>模型的<a href="../../other/application/">文档</a></p>
<p>使用这些预训练模型进行特征抽取或fine-tune的例子可以参考<a href="http://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html">此博客</a></p>
<p>VGG模型也是很多Keras例子的基础模型，如：</p>
<ul>
<li><a href="https://github.com/fchollet/keras/blob/master/examples/neural_style_transfer.py"><font color="#FF0000">Style-transfer</font></a></li>
<li><a href="https://github.com/fchollet/keras/blob/master/examples/conv_filter_visualization.py"><font color="#FF0000">Feature visualization</font></a></li>
<li><a href="https://github.com/fchollet/keras/blob/master/examples/deep_dream.py"><font color="#FF0000">Deep dream</font></a></li>
</ul>
<p><a name="hdf5">
<font color="#404040"></font></a></p><a name="hdf5"><font color="#404040">
<h2 id="kerashdf5">如何在Keras中使用HDF5输入？</h2>
</font></a><p><a name="hdf5"><font color="#404040"></font>
</a></p>
<p>你可以使用keras.utils中的<code>HDF5Matrix</code>类来读取HDF5输入，参考<a href="../../utils/">这里</a></p>
<p>可以直接使用HDF5数据库，示例</p>
<pre><code class="python hljs"><span class="hljs-keyword">import</span> h5py
<span class="hljs-keyword">with</span> h5py.File(<span class="hljs-string">'input/file.hdf5'</span>, <span class="hljs-string">'r'</span>) <span class="hljs-keyword">as</span> f:
    X_data = f[<span class="hljs-string">'X_data'</span>]
    model.predict(X_data)
</code></pre>

<hr>
<p><a name="where_config">
<font color="#404040"></font></a></p><a name="where_config"><font color="#404040">
<h2 id="keras_4">Keras的配置文件存储在哪里？</h2>
</font></a><p><a name="where_config"><font color="#404040"></font>
</a></p>
<p>所有的Keras数据默认存储在：</p>
<pre><code class="bash hljs"><span class="hljs-variable">$HOME</span>/.keras/
</code></pre>

<p>对windows用户而言，<code>$HOME</code>应替换为<code>%USERPROFILE%</code></p>
<p>当Keras无法在上面的位置创建文件夹时（例如由于权限原因），备用的地址是<code>/tmp/.keras/</code></p>
<p>Keras配置文件为JSON格式的文件，保存在<code>$HOME/.keras/keras.json</code>。默认的配置文件长这样：</p>
<pre><code class="hljs json">{
    "<span class="hljs-attribute">image_data_format</span>": <span class="hljs-value"><span class="hljs-string">"channels_last"</span></span>,
    "<span class="hljs-attribute">epsilon</span>": <span class="hljs-value"><span class="hljs-number">1e-07</span></span>,
    "<span class="hljs-attribute">floatx</span>": <span class="hljs-value"><span class="hljs-string">"float32"</span></span>,
    "<span class="hljs-attribute">backend</span>": <span class="hljs-value"><span class="hljs-string">"tensorflow"</span>
</span>}
</code></pre>

<p>该文件包含下列字段：</p>
<ul>
<li>默认的图像数据格式<code>channels_last</code>或<code>channels_first</code></li>
<li>用于防止除零错误的<code>epsilon</code></li>
<li>默认的浮点数类型</li>
<li>默认的后端</li>
</ul>
<p>类似的，缓存的数据集文件，即由<code>get_file()</code>下载的文件，默认保存在<code>$HOME/.keras/datasets/</code></p>
<hr>
<p><a name="reproduce">
<font color="#404040"></font></a></p><a name="reproduce"><font color="#404040">
<h2 id="keras_5">在使用Keras开发过程中，我如何获得可复现的结果?</h2>
</font></a><p><a name="reproduce"><font color="#404040"></font>
</a></p>
<p>在开发模型中，有时取得可复现的结果是很有用的。例如，这可以帮助我们定位模型性能的改变是由模型本身引起的还是由于数据上的变化引起的。下面的代码展示了如何获得可复现的结果，该代码基于Python3的tensorflow后端</p>
<pre><code class="python hljs"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">import</span> random <span class="hljs-keyword">as</span> rn

<span class="hljs-comment"># The below is necessary in Python 3.2.3 onwards to</span>
<span class="hljs-comment"># have reproducible behavior for certain hash-based operations.</span>
<span class="hljs-comment"># See these references for further details:</span>
<span class="hljs-comment"># https://docs.python.org/3.4/using/cmdline.html#envvar-PYTHONHASHSEED</span>
<span class="hljs-comment"># https://github.com/fchollet/keras/issues/2280#issuecomment-306959926</span>

<span class="hljs-keyword">import</span> os
os.environ[<span class="hljs-string">'PYTHONHASHSEED'</span>] = <span class="hljs-string">'0'</span>

<span class="hljs-comment"># The below is necessary for starting Numpy generated random numbers</span>
<span class="hljs-comment"># in a well-defined initial state.</span>

np.random.seed(<span class="hljs-number">42</span>)

<span class="hljs-comment"># The below is necessary for starting core Python generated random numbers</span>
<span class="hljs-comment"># in a well-defined state.</span>

rn.seed(<span class="hljs-number">12345</span>)

<span class="hljs-comment"># Force TensorFlow to use single thread.</span>
<span class="hljs-comment"># Multiple threads are a potential source of</span>
<span class="hljs-comment"># non-reproducible results.</span>
<span class="hljs-comment"># For further details, see: https://stackoverflow.com/questions/42022950/which-seeds-have-to-be-set-where-to-realize-100-reproducibility-of-training-res</span>

session_conf = tf.ConfigProto(intra_op_parallelism_threads=<span class="hljs-number">1</span>, inter_op_parallelism_threads=<span class="hljs-number">1</span>)

<span class="hljs-keyword">from</span> keras <span class="hljs-keyword">import</span> backend <span class="hljs-keyword">as</span> K

<span class="hljs-comment"># The below tf.set_random_seed() will make random number generation</span>
<span class="hljs-comment"># in the TensorFlow backend have a well-defined initial state.</span>
<span class="hljs-comment"># For further details, see: https://www.tensorflow.org/api_docs/python/tf/set_random_seed</span>

tf.set_random_seed(<span class="hljs-number">1234</span>)

sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)
K.set_session(sess)

<span class="hljs-comment"># Rest of code follows ...</span>
</code></pre>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../keras_linux/" class="btn btn-neutral float-right" title="Keras安装和配置指南(Linux)">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../concepts/" class="btn btn-neutral" title="一些基本概念"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr><div><div id="rtd-pmrqxukv" class="ethical-rtd"><div class="ethical-footer"><div class="ethical-content"><a href="https://readthedocs.org/sustainability/click/294/ry1eAN98yuSR/" rel="nofollow" target="_blank" class="ethical-image-link"><img src="https://readthedocs.org/sustainability/view/294/ry1eAN98yuSR/"></a><div class="ethical-text">A complete cloud platform designed for developers.<br><a href="https://readthedocs.org/sustainability/click/294/ry1eAN98yuSR/" rel="nofollow" target="_blank">Try it free - $100 credit</a></div></div><div class="ethical-callout"><small><em><a href="https://readthedocs.org/sustainability/advertising/">Sponsored</a><span> · </span><a href="https://docs.readthedocs.io/en/latest/ethical-advertising.html">Ads served ethically</a></em></small></div></div></div><hr></div>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      <span class="fa fa-book"> Read the Docs</span>
      <span class="fa fa-caret-down"></span>
    </span>
    <div class="rst-other-versions"><!-- Inserted RTD Footer -->
<div class="injected">

  

      
      
      
      <dl>
        <dt>Versions</dt>
        
         <strong> 
        <dd><a href="https://keras-cn.readthedocs.io/en/latest/for_beginners/FAQ/">latest</a></dd>
         </strong> 
        
      </dl>
      
      

      
      
      <dl>
        <dt>Downloads</dt>
        
        <dd><a href="//readthedocs.org/projects/keras-cn/downloads/pdf/latest/">PDF</a></dd>
        
        <dd><a href="//readthedocs.org/projects/keras-cn/downloads/htmlzip/latest/">HTML</a></dd>
        
        <dd><a href="//readthedocs.org/projects/keras-cn/downloads/epub/latest/">Epub</a></dd>
        
      </dl>
      
      

      
      <dl>
        <!-- These are kept as relative links for internal installs that are http -->
        <dt>On Read the Docs</dt>
        <dd>
          <a href="//readthedocs.org/projects/keras-cn/">Project Home</a>
        </dd>
        <dd>
          <a href="//readthedocs.org/projects/keras-cn/builds/">Builds</a>
        </dd>
        <dd>
          <a href="//readthedocs.org/projects/keras-cn/downloads/">Downloads</a>
        </dd>
      </dl>
      

      

      
      <dl>
        <dt>On GitHub</dt>
        <dd>
          <a href="https://github.com/MoyanZitto/keras-cn/blob/master/home/docs/checkouts/readthedocs.org/user_builds/keras-cn/checkouts/latest/docs/for_beginners/FAQ.md">View</a>
        </dd>
        <dd>
          <a href="https://github.com/MoyanZitto/keras-cn/edit/master/home/docs/checkouts/readthedocs.org/user_builds/keras-cn/checkouts/latest/docs/for_beginners/FAQ.md">Edit</a>
        </dd>
      </dl>
      
      

      
      <dl>
        <dt>Search</dt>
        <dd>
          <div style="padding: 6px;">
            <form id="flyout-search-form" class="wy-form" target="_blank" action="//readthedocs.org/projects/keras-cn/search/" method="get">
              <input type="text" name="q" placeholder="Search docs">
              </form>
          </div>
        </dd>
      </dl>
      



      <hr>
      
        <small>
          <span>Hosted by <a href="https://readthedocs.org">Read the Docs</a></span>
          <span> · </span>
          <a href="https://docs.readthedocs.io/en/latest/privacy-policy.html">Privacy Policy</a>
        </small>
      

      

</div>
</div>
  </div>



</body></html>